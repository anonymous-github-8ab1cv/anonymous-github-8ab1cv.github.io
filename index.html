<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Delphi</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <style>
    body {
      font-family: Arial, sans-serif;
      padding: 20px;
      /* background-color: #f5f5f5; */
    }    
    .container {
      width: 90%;
      max-width: 800px;
      text-align: center;
    }

    h2 {
      margin-bottom: 20px;
    }

    /* å†…å®¹å— */
    .content-block {
      padding: 20px;
    }

    /* å¥‡æ•°å†…å®¹å—èƒŒæ™¯è‰²ä¸ºç°è‰² */
    .content-block:nth-child(odd) {
      background-color: #f0f0f0; /* æµ…ç°è‰² */
    }

    /* å¶æ•°å†…å®¹å—èƒŒæ™¯è‰²ä¸ºç™½è‰² */
    .content-block:nth-child(even) {
      background-color: #ffffff; /* ç™½è‰² */
    }

    /* å›¾ç‰‡å±•ç¤ºåŒºåŸŸ */
    .image-display {
      position: relative;
      width: 100%;
      /* ç§»é™¤å›ºå®šé«˜åº¦ï¼Œè®©é«˜åº¦æ ¹æ®å›¾ç‰‡è‡ªåŠ¨è°ƒæ•´ */
      border: 1px solid #ccc;
      background-color: #fff;
      overflow: hidden;
    }

    .image-display img {
      width: 100%;
      height: auto; /* ä¿æŒå›¾ç‰‡æ¯”ä¾‹ */
      display: none;
      position: absolute;
      top: 0;
      left: 0;
    }

    .image-display img.active {
      display: block;
      position: relative; /* ä½¿å›¾ç‰‡åœ¨æ­£å¸¸æµä¸­æ˜¾ç¤ºï¼Œé¿å…ç»å¯¹å®šä½å¸¦æ¥çš„è¦†ç›–é—®é¢˜ */
    }

    /* æ»‘åŠ¨æ¡å®¹å™¨ */
    .slider-container {
      margin-top: 20px;
    }

    /* è‡ªå®šä¹‰æ»‘åŠ¨æ¡æ ·å¼ */
    input[type="range"] {
      -webkit-appearance: none; /* ç§»é™¤é»˜è®¤æ ·å¼ */
      width: 100%;
      height: 15px; /* å¢åŠ æ»‘åŠ¨æ¡é«˜åº¦ */
      background: #d3d3d3;
      border-radius: 5px;
      outline: none;
      margin: 0;
      padding: 0;
    }

    /* é’ˆå¯¹Webkitæµè§ˆå™¨ï¼ˆChromeã€Safariç­‰ï¼‰ */
    input[type="range"]::-webkit-slider-thumb {
      -webkit-appearance: none;
      appearance: none;
      width: 25px; /* å¢å¤§æ»‘å—å¤§å° */
      height: 25px;
      background: #4CAF50;
      cursor: pointer;
      border-radius: 50%;
      border: none;
      margin-top: -5px; /* ä½¿æ»‘å—å‚ç›´å±…ä¸­ */
    }

    input[type="range"]::-webkit-slider-runnable-track {
      height: 15px;
      cursor: pointer;
      background: #d3d3d3;
      border-radius: 5px;
    }

    /* é’ˆå¯¹Firefoxæµè§ˆå™¨ */
    input[type="range"]::-moz-range-thumb {
      width: 25px;
      height: 25px;
      background: #4CAF50;
      cursor: pointer;
      border-radius: 50%;
      border: none;
    }

    input[type="range"]::-moz-range-track {
      height: 15px;
      cursor: pointer;
      background: #d3d3d3;
      border-radius: 5px;
    }

    /* é’ˆå¯¹IEæµè§ˆå™¨ */
    input[type="range"]::-ms-thumb {
      width: 25px;
      height: 25px;
      background: #4CAF50;
      cursor: pointer;
      border-radius: 50%;
      border: none;
    }

    input[type="range"]::-ms-track {
      height: 15px;
      cursor: pointer;
      background: transparent; 
      border-color: transparent;
      color: transparent;
    }

    input[type="range"]::-ms-fill-lower {
      background: #d3d3d3;
      border-radius: 5px;
    }

    input[type="range"]::-ms-fill-upper {
      background: #d3d3d3;
      border-radius: 5px;
    }

    /* ä¸ºæ»‘åŠ¨æ¡æ·»åŠ è¿‡æ¸¡æ•ˆæœï¼ˆå¯é€‰ï¼‰ */
    input[type="range"] {
      transition: background 0.3s;
    }

    input[type="range"]:hover {
      background: #c0c0c0;
    }
    /* å“åº”å¼å›¾ç‰‡å±•ç¤ºä¼˜åŒ– */
    @media (max-width: 600px) {
      .image-display {
        border: none; /* ç§»é™¤è¾¹æ¡†ï¼ŒèŠ‚çœç©ºé—´ */
      }
    }
  
  .image-display img {
    transition: opacity 0.5s ease;
    opacity: 0;
  }

  .image-display img.active {
    opacity: 1;
    position: relative;
  }

  </style>

</head>
<body>


  <section class="hero content-block">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- <h1 class="title is-1 publication-title">Unleashing Generalization of End-to-End Autonomous Driving with Controllable Long Video Generation</h1> -->
            <h1 class="title is-1 publication-title">Delphi: Highly Controllable World Model for Unleashing Generalization of End-to-End Autonomous Driving</h1>
            <!-- <div class="is-size-5 publication-authors"> -->
              <!-- Paper authors -->
              <!-- <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">First Author</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Second Author</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Third Author</a>
                  </span> -->
                  <!-- </div> -->

                  <div class="is-size-5 publication-authors">
                    <!-- <span class="author-block">Institution Name<br>Conferance name and year</span> -->
                    <span class="author-block">ICLR 2025 Anonymous Submission</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <!-- <div class="column has-text-centered">
                    <div class="publication-links"> -->
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            <!-- </div>
          </div> -->
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero  content-block">
  <div class="container is-max-desktop">
    <!-- <div class="hero-body"> -->
      <h2 class="title is-3">Method Overview</h2>
    <div class="item">
      <img src="static/images/teaser.png" alt="MY ALT TEXT" />
      <h2 style="font-size: 18px" class="subtitle has-text-centered">
        We show that <b>(a)</b> our <i>Delphi</i> can generate up to 40 frames consecutive videos while <b>(b)</b> existing best only generate 8 frames. <b>(c)</b> With the failure-cased driven framework equipped with <i>Delphi</i>, <b>(d)</b> we can significantly boost the end-to-end model performance with much smaller cost.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Using generative models to synthesize new data has become a de-facto standard in autonomous driving to address the data scarcity issue. Though existing approaches are able to boost perception models, we discover that these approaches fail to improve the performance of planning of end-to-end autonomous driving models as the generated videos are usually less than 8 frames and the spatial and temporal inconsistencies are not negligible. To this end, we propose <i>Delphi</i>, a novel diffusion-based long video generation method with a shared noise modeling mechanism across the multi-views to increase spatial consistency, and a feature-aligned module to achieves both precise controllability and temporal consistency. Our method can generate up to 40 frames of video without loss of consistency which is about 5 times longer compared with state-of-the-art methods. Instead of randomly generating new data, we further design a sampling policy to let <i>Delphi</i> generate new data that are similar to those failure cases to improve the sample efficiency. This is achieved by building a failure-case driven framework with the help of pre-trained visual language models. Our extensive experiment demonstrates that our <i>Delphi</i> generates a higher quality of long videos surpassing previous state-of-the-art methods. Consequentially, with only generating 4% of the training dataset size, our framework is able to go beyond perception and prediction task, for the first time to the best of our knowledge, boost the planning performance of the end-to-end autonomous driving model by a margin of 25%.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero content-block">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/delphi-framework-small.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Overview of <i>Delphi</i>.
          <!-- <b>(a) Architecture of <i>Delphi</i></b>.  -->
          <!-- It takes multi-view videos z and the corresponding BEV (Bird's Eye View) layout sequences as input. Each video consists of N frames and V views. The BEV layout sequences are first projected into camera space according to camera parameters, resulting in camera layouts that include both foreground and background layouts. 
          Specifically, the foreground layout includes the bounding box's corner coordinates, heading, instance id, and dense caption, while the background one includes different colored lines to represent road trends. 
          The layout embeddings, processed by the encoder, are injected into the U-Net through cross-attention to achieve fine-grained layout control in the generation process. Additionally, we leverage VLM to extract dense captions for the input scenes,  which are encoded by Long-CLIP to obtain text embeddings, which are then injected into the U-Net via text cross-attention to achieve text-based control. 
          We further design two key modules,  
          <b>(b) Noise Reinitialization Module</b> that encompass a share noise across different views  and  <b>(c) Feature-aligned Temporal Consistency Module</b> to ensure spatial and temporal consistency accordingly. -->
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/failure-case-driven framwork.jpeg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Overview of Failure-case Driven Framework.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<section class="hero content-block>
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Long video generation on nuScenes dataset</h2>


      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
        <div class="item video-container">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/sunny_1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item">
          <h2 class="subtitle has-text-centered">
            ğŸ“Boston, â˜€ï¸sunny, ğŸ”†daytime
          </h2>
        </div>


        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/sunny_2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item">
          <h2 class="subtitle has-text-centered">
            ğŸ“Boston, â˜€ï¸sunny, ğŸ”†daytime
          </h2>
        </div>


        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/sunny_3.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item">
          <h2 class="subtitle has-text-centered">
            ğŸ“Boston, â˜€ï¸sunny, ğŸ”†daytime
          </h2>
        </div>

        
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/rainy_3.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item">
          <h2 class="subtitle has-text-centered">
            ğŸ“Boston, ğŸŒ§ï¸rainy, ğŸ”†daytime
          </h2>
        </div>

        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/night_3.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item">
          <h2 class="subtitle has-text-centered">
            ğŸ“Boston, â˜€ï¸sunny, ğŸŒ™night
          </h2>
        </div>
        <!-- <div class="content has-text-justified">
          
        </div> -->

      <!-- </div> -->

    </div>
  </div>
</section>
<!-- End video carousel -->

<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Precise controllability</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <!-- <div class="item">

          </div> -->
          <div class="item">
            <h2 class="subtitle has-text-centered">
              1. Visual comparison of local region generated by different generative models. 
              <!-- Our method maintains consistent spatial and temporal appearance where the previous methods fail. -->
            </h2>
            <!-- Your image here -->
            <img src="static/images/results/visual conparison of local region.png" alt="MY ALT TEXT"/>

          </div>

          <div class="item">
            
          </div>
          <div class="item"><br>
            <h2 class="subtitle has-text-centered">
              2. Precise element editing, including instance-level editing and scene-level editing.
            </h2>
            <!-- Your image here -->
            <img src="static/images/results/visual_precise_control.png" alt="MY ALT TEXT"/>

          </div>

          <div class="content has-text-justified">
          </div>
          <!-- <div id="results-carousel" class="carousel results-carousel"> -->
            <div class="item item-video1">
              <h2 class="subtitle has-text-centered">
                3. GUI Interface for users to edit layout from scratch.
              </h2>
              <video poster="" id="video1" autoplay controls muted loop height="100%">
                <!-- Your video file here -->
                <source src="static/videos/demo_layout_editing1.mp4"
                type="video/mp4">
              </video>


            </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero content-block">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Our failure-case driven framework boosts the end-to-end planning model</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="item">
            <h2 class="subtitle has-text-centered">
              1. Performance comparison of the end-to-end models fine-tuned from the UniAD open source model by applying different data sampling strategies, numbers of data cases, data engines, and data sources in the failure-case driven framework.
            </h2>
            <!-- Your image here -->
            <img src="static/images/downstream/Tab2.jpeg" alt="MY ALT TEXT"/>

          </div>
          
          <div class="item"><br>
            <h2 class="subtitle has-text-centered">
              
              2. Visualization of four examples before and after. 
              In the left, we show four hard examples from the validation set, ''large objects in the front'' and ''unprotected left turn at intersection''. In the right, our framework is able to fix these four examples without using these data during training. 
            </h2>
            <!-- Your image here -->
            <img src="static/images/downstream/uniad_generalization_comparison.png" alt="MY ALT TEXT"/>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Generation results on other dataset</h2>

      <div class="item">
        <h2  class="subtitle has-text-centered">
          We further demonstrate that <i>Delphi</i> generates spatiotemporally consistent long videos on another private dataset, which  demonstrates it's scalability.
        </h2>
      </div>

      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
        <div class="item item-video1">
          <video poster="" id="video1" playsinline autoplay controls muted loop height="100%">
            <source src="static/videos/liauto_1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" playsinline autoplay controls muted loop height="100%">
            <source src="static/videos/liauto_2.mp4"
            type="video/mp4">
          </video>
        </div>
      <!-- </div> -->
    </div>
  </div>
</section>
<!-- End video carousel -->


<!-- Video carousel -->
<section class="hero content-block">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Application: visual renderer for closed-loop evaluation</h2>

      <div class="item">
        <h2   class="subtitle has-text-centered">
          <i>Delphi</i> can also be used as a data engine with photorealistic image generation capabilities and further supports closed-loop evaluation of end-to-end models such as UniAD. Here we show a demo of closed-loop evaluation on nuNcenes. The top row shows an open-loop evaluation scenario on nuNcenes: the ego car is driving at a constant speed. The bottom row shows a closed-loop evaluation scenario on nuNcenes using Delphi: the ego car is accelerating, resulting in a dangerous distance from the vehicle in front. 
        </h2>
      </div>
    </div>
  </div>


<div class="container">

  
  <!-- å›¾ç‰‡å±•ç¤ºåŒºåŸŸ -->
  <div class="image-display" id="imageDisplay">
    <!-- ç¤ºä¾‹å›¾ç‰‡ï¼Œè¯·æ ¹æ®å®é™…æƒ…å†µæ›¿æ¢å›¾ç‰‡è·¯å¾„ -->
    <img src="static/images/frame_0001.png" class="active" alt="å›¾ç‰‡1">
    <img src="static/images/frame_0002.png" alt="å›¾ç‰‡2">
    <img src="static/images/frame_0003.png" alt="å›¾ç‰‡3">
    <img src="static/images/frame_0004.png" alt="å›¾ç‰‡4">
    <img src="static/images/frame_0005.png" alt="å›¾ç‰‡5">
    <img src="static/images/frame_0006.png" alt="å›¾ç‰‡6">
    <img src="static/images/frame_0007.png" alt="å›¾ç‰‡7">
    <img src="static/images/frame_0008.png" alt="å›¾ç‰‡8">
    <img src="static/images/frame_0009.png" alt="å›¾ç‰‡9">
    <img src="static/images/frame_0010.png" alt="å›¾ç‰‡10">
    <img src="static/images/frame_0011.png" alt="å›¾ç‰‡11">
    <img src="static/images/frame_0012.png" alt="å›¾ç‰‡12">
    <!-- å¯ä»¥ç»§ç»­æ·»åŠ æ›´å¤šå›¾ç‰‡ -->
  </div>
  
  <!-- æ»‘åŠ¨æ¡æ§åˆ¶ -->
  <div class="slider-container">
    <input type="range" id="slider" min="0" max="4" step="1" value="0">
  </div>
  <h2>Drag the slider to see how the environment rendered by <i>Delphi</i> changes with the route planned by UniAD.</h2>
</div>

<script>
  // è·å–æ‰€æœ‰å›¾ç‰‡å…ƒç´ 
  const images = document.querySelectorAll('#imageDisplay img');
  const slider = document.getElementById('slider');
  
  // åŠ¨æ€è®¾ç½®æ»‘åŠ¨æ¡çš„æœ€å¤§å€¼ï¼Œæ ¹æ®å›¾ç‰‡æ•°é‡
  slider.max = images.length - 1;
  
  // ç›‘å¬æ»‘åŠ¨æ¡çš„è¾“å…¥äº‹ä»¶
  slider.addEventListener('input', function() {
    const index = parseInt(this.value); // è·å–å½“å‰æ»‘åŠ¨æ¡çš„å€¼å¹¶è½¬ä¸ºæ•´æ•°
    images.forEach((img, i) => {
      if (i === index) {
        img.classList.add('active'); // å½“å‰ç´¢å¼•çš„å›¾ç‰‡æ˜¾ç¤º
      } else {
        img.classList.remove('active'); // å…¶ä»–å›¾ç‰‡éšè—
      }
    });
  });
</script>

</section>
<!-- End video carousel -->


<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
    
              <p>
                This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
                This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>
    
            </div>
          </div>
        </div>
      </div>
    </footer>

  </body>
  </html>
